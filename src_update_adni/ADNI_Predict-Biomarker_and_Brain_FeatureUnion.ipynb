{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     26
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pickle\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "import scipy.stats as st\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "from numpy import *\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,cross_val_predict,StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc,f1_score\n",
    "\n",
    "##read the data and clean data\n",
    "def data_fetch_clean(file):\n",
    "    #os.getcwd()\n",
    "    #os.chdir('../braindata')\n",
    "    dd =pd.read_csv(file,header=0)\n",
    "    print(dd.shape)\n",
    "    colnames=list(dd)\n",
    "    conn_start_index=colnames.index('T1')\n",
    "    import csv\n",
    "\n",
    "    with open(file, 'r') as f:\n",
    "        d_reader = csv.DictReader(f)\n",
    "\n",
    "        #get fieldnames from DictReader object and store in list\n",
    "        headers = d_reader.fieldnames\n",
    "    data=np.array(dd)\n",
    "    #print(data.shape)\n",
    "    idx_IN_columns = np.append(np.array([8,9]),np.array(range(1,6)))# 8 and 9 is Final dx and time to dimentia\n",
    "    dd=dd[~dd.iloc[:,idx_IN_columns].isnull().any(axis=1)]\n",
    "    print(dd.shape)\n",
    "   #idx_IN_columns = np.append(np.array([3]),np.array(range(11,data.shape[1])))\n",
    "    dd=dd[dd['dxbl']==2]#2 is MCI group\n",
    "    print(dd.shape)\n",
    "    X_brain=dd.iloc[:,conn_start_index:]\n",
    "    \n",
    "    #print(idx_IN_columns)\n",
    "    XX=dd.iloc[:,idx_IN_columns] # only MCI group with biomarker data\n",
    "    #features=data[:,11:data.shape[1]]\n",
    "    #features = features.transpose()\n",
    "    ind_x_num=np.isnan(XX)\n",
    "    XX=XX.dropna()  \n",
    "    X_biomarker = stats.zscore(XX.iloc[:,2:7])\n",
    "    \n",
    "    \n",
    "    #print(features.shape)\n",
    "    y=XX.iloc[:,1]\n",
    "#/ 6:AD-normal / 7:AD-MCI / 8:MCI-normal \n",
    "\n",
    "\n",
    "#     ind_num=np.isnan(y)\n",
    "#     # print(ind_num.shape)\n",
    "\n",
    "\n",
    "#     y_no_nan = y[~ind_num]\n",
    "\n",
    "#     X_no_nan = X[~ind_num,:]\n",
    "\n",
    "#            # print(y.shape)\n",
    "    X_biomarker[np.isnan(X_biomarker)] = np.median(X_biomarker[~np.isnan(X_biomarker)])\n",
    "    X_brain[np.isnan(X_brain)] = np.median(X_brain[~np.isnan(X_brain)])\n",
    "    print(np.isnan(X_brain))\n",
    "    X={'Biomarker':X_biomarker,'Brain': X_brain}\n",
    "#     y=y_no_nan\n",
    "#     X=X_no_nan\n",
    "#     feature_num_all=[]\n",
    "#     lr_all_feature=[]\n",
    "#     svm_all_feature=[]\n",
    "#     lr_fls_feature=[]\n",
    "#     svm_fls_feature=[]\n",
    "#     base_labels= []\n",
    "\n",
    "    #np.isnan(X).any()\n",
    "    #X[np.isnan(X)] = np.median(X[~np.isnan(X)])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# os.chdir('../data/adni')\n",
    "# filename='combine_new_biomarker_correct'\n",
    "# file=filename+'.csv'\n",
    "\n",
    "# dd =pd.read_csv(file,header=0)\n",
    "# idx_IN_columns = np.array(range(1,6))# 8 and 9 is Final dx and time to dimentia\n",
    "# dd=dd[~dd.iloc[:,idx_IN_columns].isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_footer\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_quoting\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=np.array(dd)\n",
    "#     #print(data.shape)\n",
    "# idx_IN_columns = np.array(range(1,6))\n",
    "# # idx_IN_columns = np.append(np.array([3]),np.array(range(11,data.shape[1])))\n",
    "# dd=dd[dd['dxbl']==2]\n",
    "# #print(idx_filter)\n",
    "# X=dd.iloc[:,idx_IN_columns]\n",
    "# ind_x_num=np.isnan(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def main_classifier(Z,y,name,filename,params,pipe,path_to_save,key):\n",
    "    all_TP = []\n",
    "    all_TN = []\n",
    "    all_FP = []\n",
    "    all_FN = []\n",
    "\n",
    "    all_acc = []\n",
    "    all_sen = []\n",
    "    all_spec = []\n",
    "    all_auc = []\n",
    "\n",
    "    all_roc_label = []\n",
    "    all_roc_pred = []\n",
    "    all_roc_prob = []\n",
    "    all_features=[]\n",
    "    n_fold = 10\n",
    "    #rs_list=[33994,31358,27381,8642,7012,42023,44642,44002,30706,12571]\n",
    "    #rs_list=[33994]\n",
    "    rs_list=[33994,31358,27381]\n",
    "    for rs in rs_list:\n",
    "        print('********random seed:{}'.format(rs))\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=rs)\n",
    "        outer_cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=rs)\n",
    "\n",
    "        combine_pipe=Pipeline([\n",
    "            ('union', FeatureUnion(\n",
    "                    transformer_list=[\n",
    "                            # Pipeline for pulling features from the post's subject line\n",
    "                            ('Biomarker', Pipeline([\n",
    "                                ('selector', ItemSelector(key='Biomarker')),\n",
    "                    \n",
    "\n",
    "                            ])),\n",
    "                            ('Brain', Pipeline([\n",
    "                                ('selector', ItemSelector(key='Brain')),\n",
    "                                ('PCA', PCA()),\n",
    "                            ])),\n",
    "\n",
    "                        ],\n",
    "            # weight components in FeatureUnion\n",
    "                    transformer_weights={\n",
    "                                'Biomarker': 0.8,\n",
    "                                'Brain': 0.5,\n",
    "                                },\n",
    "            ))])\n",
    "        X=combine_pipe.fit_transform()\n",
    "        avg_auc = []\n",
    "        avg_acc = []\n",
    "        avg_TP = []\n",
    "        avg_TN = []\n",
    "        avg_FP = []\n",
    "        avg_FN = []\n",
    "        avg_sen = []\n",
    "        avg_spec = []\n",
    "     \n",
    "        roc_label = []\n",
    "        roc_pred = []\n",
    "        roc_prob = []\n",
    "        features=[]\n",
    "        for train_index, test_index in outer_cv.split(X,y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            # 'featureExtract__n_estimators': np.arange(10, 100, 10),\n",
    "#             params = {'randomforest__min_samples_leaf': np.arange(1, 51, 5),\n",
    "#                       'randomforest__n_estimators': np.arange(10, 500, 10)}\n",
    "#             # clf_m = RandomForestClassifier(random_state=0)\n",
    "\n",
    "#             pipe = Pipeline([\n",
    "#                 ('featureExtract', SelectFromModel(ExtraTreesClassifier())),\n",
    "#                 ('randomforest', RandomForestClassifier())\n",
    "#             ])\n",
    "\n",
    "            clf = GridSearchCV(estimator=pipe, param_grid=params, cv=inner_cv, scoring='accuracy',n_jobs=1)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            fs = clf.best_estimator_.named_steps['featureExtract']\n",
    "            mask = fs.get_support(indices=True)\n",
    "            print(mask.shape)\n",
    "            #features=np.append(features,mask[:10,])\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_prob = clf.predict_proba(X_test)\n",
    "          #  roc_pred = clf.predict_proba(X_test)\n",
    "\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
    "\n",
    "            roc_label = np.append(roc_label, y_test)\n",
    "            roc_pred = np.append(roc_pred, y_pred)\n",
    "            roc_prob = np.append(roc_prob, y_prob[:, 1])\n",
    "\n",
    "\n",
    "            conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "            TP = conf_mat[0][0]\n",
    "            FP = conf_mat[0][1]\n",
    "            FN = conf_mat[1][0]\n",
    "            TN = conf_mat[1][1]\n",
    "\n",
    "            avg_TP = np.append(avg_TP, TP)\n",
    "            avg_TN = np.append(avg_TN, TN)\n",
    "            avg_FP = np.append(avg_FP, FP)\n",
    "            avg_FN = np.append(avg_FN, FN)\n",
    "\n",
    "            avg_acc = np.append(avg_acc, acc)\n",
    "\n",
    "            print(TP, FP, FN, TN)\n",
    "            sen = TP / (TP + FN)\n",
    "            spec = TN / (TN + FP)\n",
    "\n",
    "            avg_sen = np.append(avg_sen, sen)\n",
    "            avg_spec = np.append(avg_spec, spec)\n",
    "            avg_auc = np.append(avg_auc, auc)\n",
    "        \n",
    "#             print('Accuracy:{},AUC:{}'.format(acc, auc))\n",
    "#             print('Sensitivity:{},Specificity:{}'.format(sen, spec))\n",
    "#             print('Accuracy:{},AUC:{}'.format(acc, auc))\n",
    "#             print('Sensitivity:{},Specificity:{}'.format(sen, spec))\n",
    "        all_TP = np.append(all_TP, avg_TP)\n",
    "        all_TN = np.append(all_TN, avg_TN)\n",
    "        all_FP = np.append(all_FP, avg_FP)\n",
    "        all_FN = np.append(all_FN, avg_FN)\n",
    "\n",
    "        all_acc = np.append(all_acc, avg_acc)\n",
    "        all_sen = np.append(all_sen, avg_sen)\n",
    "        all_spec = np.append(all_spec, avg_spec)\n",
    "        all_auc = np.append(all_auc, avg_auc)\n",
    "\n",
    "        all_roc_label = np.append(all_roc_label, roc_label)\n",
    "        all_roc_pred = np.append(all_roc_pred, roc_pred)\n",
    "        all_roc_prob = np.append(all_roc_prob, roc_prob)\n",
    "        all_features=np.append(all_features,features)\n",
    "#         print(\"Accuracy Avg: {}\".format(np.mean(avg_acc)))\n",
    "#         print(\"Accuracy Standard Deviation: {}\".format(np.std(avg_acc)))\n",
    "#         print(\"Sensitivity Avg: {}\".format(np.mean(avg_sen)))\n",
    "#         print(\"Sensitivity Standard Deviation: {}\".format(np.std(avg_sen)))\n",
    "#         print(\"Specificity Avg: {}\".format(np.mean(avg_spec)))\n",
    "#         print(\"Specificity Standard Deviation: {}\".format(np.std(avg_spec)))\n",
    "\n",
    "        #if dataset == 1:\n",
    "    pickle.dump(all_roc_label, open(path_to_save+'/'+'roc_label_'+name+'.p', \"wb\"))\n",
    "    pickle.dump(all_roc_pred, open(path_to_save+'/'+'roc_pred_'+name+'.p', \"wb\"))\n",
    "    pickle.dump(all_roc_prob, open(path_to_save+'/'+'roc_prob_'+name+'.p', \"wb\"))\n",
    "    pickle.dump(all_features, open(path_to_save+'/'+'features_30'+name+'.p', \"wb\"))\n",
    "\n",
    "    acc_CI=st.t.interval(0.95, len(all_acc)-1, loc=np.mean(all_acc), scale=st.sem(all_acc))\n",
    "    sen_CI=st.t.interval(0.95, len(all_sen)-1, loc=np.mean(all_sen), scale=st.sem(all_sen))\n",
    "    spec_CI=st.t.interval(0.95, len(all_spec)-1, loc=np.mean(all_spec), scale=st.sem(all_spec))\n",
    "    auc_CI=st.t.interval(0.95, len(all_auc)-1, loc=np.mean(all_auc), scale=st.sem(all_auc))\n",
    "    import os\n",
    "    \n",
    "#     if not os.path.exists('../imgs3_idp/'+  todaystr+'/'+filename):\n",
    "#         os.makedirs('../imgs3_idp/'+filename)\n",
    "    txt_name=path_to_save+'/'+name +  '.txt'\n",
    "    print(\"ACC={a}, 95%CI={l}-{u}\".format(a=np.mean(all_acc), l=acc_CI[0],u=acc_CI[1]),file=open(txt_name, \"a\"))\n",
    "    print(\"AUC={a}, 95%CI={l}-{u}\".format(a=np.mean(all_auc), l=auc_CI[0],u=auc_CI[1]),file=open(txt_name, \"a\"))\n",
    "    print(\"SENSITIVITY={a}, 95%CI={l}-{u}\".format(a=np.mean(all_sen), l=sen_CI[0],u=sen_CI[1]),file=open(txt_name, \"a\"))\n",
    "    print(\"SPECIFICITY={a}, 95%CI={l}-{u}\".format(a=np.mean(all_spec), l=spec_CI[0],u=spec_CI[1]),file=open(txt_name, \"a\"))\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_roc_label, all_roc_prob)\n",
    "    #auc = roc_auc_score(all_roc_label, all_roc_prob)\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(fpr, tpr, lw=2, label=key+ ' '+'(AUC = %0.2f)' % np.mean(all_auc))\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
    "    plt.xlim([-0.01, 1.01])\n",
    "    plt.ylim([-0.01, 1.01])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(name)\n",
    "    plt.legend(loc=\"lower right\") \n",
    "    #plt.savefig('10x_Combined_ROC.eps')\n",
    "    roc_name=path_to_save+'/'+name +'.pdf'\n",
    "    plt.savefig(roc_name)\n",
    "    #plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name=[\"MCI converter\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename='combine_MConly'\n",
    "filename='combine_new_biomarker_correct'\n",
    "file=filename+'.csv'\n",
    "cwd=os.getcwd()\n",
    "os.chdir('../data/adni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, datetime, os\n",
    "\n",
    "today = datetime.date.today()  \n",
    "\n",
    "todaystr = today.isoformat()  \n",
    "if not os.path.exists('../../imgs3_adni/'+todaystr):\n",
    "   # print('exist')\n",
    "    os.mkdir('../../imgs3_adni/'+ todaystr)\n",
    "    #os.mkdir('../../imgs3_adni/'+ todaystr+'/'+filename)\n",
    "if not os.path.exists('../../imgs3_adni/'+todaystr+'/'+filename):\n",
    "   # print('exist')\n",
    "    #os.mkdir('../../imgs3_adni/'+ todaystr)\n",
    "    os.mkdir('../../imgs3_adni/'+ todaystr+'/'+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "models1 = { \n",
    "    \n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'SVC': SVC(probability=True),\n",
    "    'linear_model.LogisticRegression':linear_model.LogisticRegression()\n",
    "    \n",
    "}\n",
    "\n",
    "params1 = { \n",
    "            'RandomForestClassifier': [{ 'RandomForestClassifier__n_estimators': np.arange(10, 500, 50) },\n",
    "                                       {'RandomForestClassifier__min_samples_leaf': np.arange(1, 51, 5)},\n",
    "                                      ],\n",
    "    'SVC': [\n",
    "        {'SVC__kernel': ['linear'], 'SVC__C': [0.001,0.01,0.1,1, 10]},\n",
    "    ],\n",
    "    'linear_model.LogisticRegression':{'linear_model.LogisticRegression__C':[0.001, 0.01, 0.1, 1, 10]}\n",
    "}\n",
    "\n",
    "path_save='../../imgs3_adni/' + todaystr+'/'+filename+'/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      "(179, 34733)\n",
      "(140, 34733)\n",
      "(51, 34733)\n",
      "        T1     T2     T3     T4     T5     T6     T7     T8     T9    T10  \\\n",
      "12   False  False  False  False  False  False  False  False  False  False   \n",
      "13   False  False  False  False  False  False  False  False  False  False   \n",
      "14   False  False  False  False  False  False  False  False  False  False   \n",
      "15   False  False  False  False  False  False  False  False  False  False   \n",
      "18   False  False  False  False  False  False  False  False  False  False   \n",
      "20   False  False  False  False  False  False  False  False  False  False   \n",
      "22   False  False  False  False  False  False  False  False  False  False   \n",
      "23   False  False  False  False  False  False  False  False  False  False   \n",
      "25   False  False  False  False  False  False  False  False  False  False   \n",
      "29   False  False  False  False  False  False  False  False  False  False   \n",
      "32   False  False  False  False  False  False  False  False  False  False   \n",
      "37   False  False  False  False  False  False  False  False  False  False   \n",
      "47   False  False  False  False  False  False  False  False  False  False   \n",
      "49   False  False  False  False  False  False  False  False  False  False   \n",
      "60   False  False  False  False  False  False  False  False  False  False   \n",
      "61   False  False  False  False  False  False  False  False  False  False   \n",
      "62   False  False  False  False  False  False  False  False  False  False   \n",
      "63   False  False  False  False  False  False  False  False  False  False   \n",
      "69   False  False  False  False  False  False  False  False  False  False   \n",
      "72   False  False  False  False  False  False  False  False  False  False   \n",
      "73   False  False  False  False  False  False  False  False  False  False   \n",
      "76   False  False  False  False  False  False  False  False  False  False   \n",
      "77   False  False  False  False  False  False  False  False  False  False   \n",
      "82   False  False  False  False  False  False  False  False  False  False   \n",
      "85   False  False  False  False  False  False  False  False  False  False   \n",
      "89   False  False  False  False  False  False  False  False  False  False   \n",
      "91   False  False  False  False  False  False  False  False  False  False   \n",
      "93   False  False  False  False  False  False  False  False  False  False   \n",
      "95   False  False  False  False  False  False  False  False  False  False   \n",
      "96   False  False  False  False  False  False  False  False  False  False   \n",
      "99   False  False  False  False  False  False  False  False  False  False   \n",
      "100  False  False  False  False  False  False  False  False  False  False   \n",
      "105  False  False  False  False  False  False  False  False  False  False   \n",
      "106  False  False  False  False  False  False  False  False  False  False   \n",
      "107  False  False  False  False  False  False  False  False  False  False   \n",
      "109  False  False  False  False  False  False  False  False  False  False   \n",
      "111  False  False  False  False  False  False  False  False  False  False   \n",
      "112  False  False  False  False  False  False  False  False  False  False   \n",
      "114  False  False  False  False  False  False  False  False  False  False   \n",
      "115  False  False  False  False  False  False  False  False  False  False   \n",
      "116  False  False  False  False  False  False  False  False  False  False   \n",
      "117  False  False  False  False  False  False  False  False  False  False   \n",
      "120  False  False  False  False  False  False  False  False  False  False   \n",
      "122  False  False  False  False  False  False  False  False  False  False   \n",
      "123  False  False  False  False  False  False  False  False  False  False   \n",
      "126  False  False  False  False  False  False  False  False  False  False   \n",
      "128  False  False  False  False  False  False  False  False  False  False   \n",
      "133  False  False  False  False  False  False  False  False  False  False   \n",
      "135  False  False  False  False  False  False  False  False  False  False   \n",
      "137  False  False  False  False  False  False  False  False  False  False   \n",
      "154  False  False  False  False  False  False  False  False  False  False   \n",
      "\n",
      "        ...       VarName999  VarName1000  VarName1001  VarName1002  \\\n",
      "12      ...            False        False        False        False   \n",
      "13      ...            False        False        False        False   \n",
      "14      ...            False        False        False        False   \n",
      "15      ...            False        False        False        False   \n",
      "18      ...            False        False        False        False   \n",
      "20      ...            False        False        False        False   \n",
      "22      ...            False        False        False        False   \n",
      "23      ...            False        False        False        False   \n",
      "25      ...            False        False        False        False   \n",
      "29      ...            False        False        False        False   \n",
      "32      ...            False        False        False        False   \n",
      "37      ...            False        False        False        False   \n",
      "47      ...            False        False        False        False   \n",
      "49      ...            False        False        False        False   \n",
      "60      ...            False        False        False        False   \n",
      "61      ...            False        False        False        False   \n",
      "62      ...            False        False        False        False   \n",
      "63      ...            False        False        False        False   \n",
      "69      ...            False        False        False        False   \n",
      "72      ...            False        False        False        False   \n",
      "73      ...            False        False        False        False   \n",
      "76      ...            False        False        False        False   \n",
      "77      ...            False        False        False        False   \n",
      "82      ...            False        False        False        False   \n",
      "85      ...            False        False        False        False   \n",
      "89      ...            False        False        False        False   \n",
      "91      ...            False        False        False        False   \n",
      "93      ...            False        False        False        False   \n",
      "95      ...            False        False        False        False   \n",
      "96      ...            False        False        False        False   \n",
      "99      ...            False        False        False        False   \n",
      "100     ...            False        False        False        False   \n",
      "105     ...            False        False        False        False   \n",
      "106     ...            False        False        False        False   \n",
      "107     ...            False        False        False        False   \n",
      "109     ...            False        False        False        False   \n",
      "111     ...            False        False        False        False   \n",
      "112     ...            False        False        False        False   \n",
      "114     ...            False        False        False        False   \n",
      "115     ...            False        False        False        False   \n",
      "116     ...            False        False        False        False   \n",
      "117     ...            False        False        False        False   \n",
      "120     ...            False        False        False        False   \n",
      "122     ...            False        False        False        False   \n",
      "123     ...            False        False        False        False   \n",
      "126     ...            False        False        False        False   \n",
      "128     ...            False        False        False        False   \n",
      "133     ...            False        False        False        False   \n",
      "135     ...            False        False        False        False   \n",
      "137     ...            False        False        False        False   \n",
      "154     ...            False        False        False        False   \n",
      "\n",
      "     VarName1003  VarName1004  VarName1005  VarName1006  VarName1007  \\\n",
      "12         False        False        False        False        False   \n",
      "13         False        False        False        False        False   \n",
      "14         False        False        False        False        False   \n",
      "15         False        False        False        False        False   \n",
      "18         False        False        False        False        False   \n",
      "20         False        False        False        False        False   \n",
      "22         False        False        False        False        False   \n",
      "23         False        False        False        False        False   \n",
      "25         False        False        False        False        False   \n",
      "29         False        False        False        False        False   \n",
      "32         False        False        False        False        False   \n",
      "37         False        False        False        False        False   \n",
      "47         False        False        False        False        False   \n",
      "49         False        False        False        False        False   \n",
      "60         False        False        False        False        False   \n",
      "61         False        False        False        False        False   \n",
      "62         False        False        False        False        False   \n",
      "63         False        False        False        False        False   \n",
      "69         False        False        False        False        False   \n",
      "72         False        False        False        False        False   \n",
      "73         False        False        False        False        False   \n",
      "76         False        False        False        False        False   \n",
      "77         False        False        False        False        False   \n",
      "82         False        False        False        False        False   \n",
      "85         False        False        False        False        False   \n",
      "89         False        False        False        False        False   \n",
      "91         False        False        False        False        False   \n",
      "93         False        False        False        False        False   \n",
      "95         False        False        False        False        False   \n",
      "96         False        False        False        False        False   \n",
      "99         False        False        False        False        False   \n",
      "100        False        False        False        False        False   \n",
      "105        False        False        False        False        False   \n",
      "106        False        False        False        False        False   \n",
      "107        False        False        False        False        False   \n",
      "109        False        False        False        False        False   \n",
      "111        False        False        False        False        False   \n",
      "112        False        False        False        False        False   \n",
      "114        False        False        False        False        False   \n",
      "115        False        False        False        False        False   \n",
      "116        False        False        False        False        False   \n",
      "117        False        False        False        False        False   \n",
      "120        False        False        False        False        False   \n",
      "122        False        False        False        False        False   \n",
      "123        False        False        False        False        False   \n",
      "126        False        False        False        False        False   \n",
      "128        False        False        False        False        False   \n",
      "133        False        False        False        False        False   \n",
      "135        False        False        False        False        False   \n",
      "137        False        False        False        False        False   \n",
      "154        False        False        False        False        False   \n",
      "\n",
      "     VarName1008  \n",
      "12         False  \n",
      "13         False  \n",
      "14         False  \n",
      "15         False  \n",
      "18         False  \n",
      "20         False  \n",
      "22         False  \n",
      "23         False  \n",
      "25         False  \n",
      "29         False  \n",
      "32         False  \n",
      "37         False  \n",
      "47         False  \n",
      "49         False  \n",
      "60         False  \n",
      "61         False  \n",
      "62         False  \n",
      "63         False  \n",
      "69         False  \n",
      "72         False  \n",
      "73         False  \n",
      "76         False  \n",
      "77         False  \n",
      "82         False  \n",
      "85         False  \n",
      "89         False  \n",
      "91         False  \n",
      "93         False  \n",
      "95         False  \n",
      "96         False  \n",
      "99         False  \n",
      "100        False  \n",
      "105        False  \n",
      "106        False  \n",
      "107        False  \n",
      "109        False  \n",
      "111        False  \n",
      "112        False  \n",
      "114        False  \n",
      "115        False  \n",
      "116        False  \n",
      "117        False  \n",
      "120        False  \n",
      "122        False  \n",
      "123        False  \n",
      "126        False  \n",
      "128        False  \n",
      "133        False  \n",
      "135        False  \n",
      "137        False  \n",
      "154        False  \n",
      "\n",
      "[51 rows x 34709 columns]\n",
      "********random seed:33994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/posnerlab/miniconda3/lib/python3.6/site-packages/pandas/core/series.py:842: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3198f74cdedf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#y = y.reshape(-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mmain_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpara\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-af3bda7f8af6>\u001b[0m in \u001b[0;36mmain_classifier\u001b[0;34m(Z, y, name, filename, params, pipe, path_to_save, key)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'featureExtract'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \"\"\"\n\u001b[0;32m--> 672\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "for key, value in models1.items():\n",
    "    pipe=Pipeline([\n",
    "                ('featureExtract', SelectFromModel(ExtraTreesClassifier())),\n",
    "                (key, models1[key])\n",
    "             ])\n",
    "\n",
    "    print(key)\n",
    "    para=params1[key]\n",
    "    path_to_save=path_save+key\n",
    "    if not os.path.exists(path_to_save):\n",
    "        os.mkdir(path_to_save)\n",
    "    \n",
    "   \n",
    "   \n",
    "    X,y=data_fetch_clean(file)\n",
    "\n",
    "    #y = y.reshape(-1)\n",
    "    name=save_name[0]\n",
    "    main_classifier(X,y,name,filename,para,pipe,path_to_save,key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  combine_pipe=Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#                 transformer_list=[\n",
    "#                         # Pipeline for pulling features from the post's subject line\n",
    "#                         ('Biomarker', Pipeline([\n",
    "#                             ('selector', ItemSelector(key='Biomarker')),\n",
    "\n",
    "#                         ])),\n",
    "#                         ('Brain', Pipeline([\n",
    "#                             ('selector', ItemSelector(key='Brain')),\n",
    "#                             ('PCA', PCA()),\n",
    "#                         ])),\n",
    "\n",
    "#                     ],\n",
    "#         # weight components in FeatureUnion\n",
    "#                 transformer_weights={\n",
    "#                             'Biomarker': 0.5,\n",
    "#                             'Brain': 0.5,\n",
    "#                             },\n",
    "#         ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
