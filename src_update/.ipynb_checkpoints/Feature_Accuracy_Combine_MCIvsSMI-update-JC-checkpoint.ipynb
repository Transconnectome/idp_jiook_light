{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn import linear_model, svm\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, auc,f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#s = \"../braindata/data_1_mor_select_100.csv\"\n",
    "import os          \n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\stats.py:2248: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (a - mns) / sstd\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir('../braindata')\n",
    "dd =pd.read_csv(\"data_3_all.csv\",header=0)\n",
    "import csv\n",
    "\n",
    "with open('data_3_all.csv', 'r') as f:\n",
    "    d_reader = csv.DictReader(f)\n",
    "\n",
    "    #get fieldnames from DictReader object and store in list\n",
    "    headers = d_reader.fieldnames\n",
    "data=np.array(dd)\n",
    "#print(data.shape)\n",
    "idx_IN_columns = np.append(np.array([3,4]),np.array(range(11,data.shape[1])))\n",
    "X=data[:,idx_IN_columns]\n",
    "#features=data[:,11:data.shape[1]]\n",
    "#features = features.transpose()\n",
    "X = stats.zscore(X)\n",
    "#print(features.shape)\n",
    "y=data[:,6]\n",
    "#5: ad-smi / 6:mci-smi / 7:adonly-smi / 8:ad-mci / 9:adonly-mci / 10:adonly - adwithsmallvv\n",
    "\n",
    "ind_num=np.isnan(y)\n",
    "# print(ind_num.shape)\n",
    "\n",
    "\n",
    "y_no_nan = y[~ind_num]\n",
    "\n",
    "X_no_nan = X[~ind_num,:]\n",
    "\n",
    "       # print(y.shape)\n",
    "\n",
    "y=y_no_nan\n",
    "X=X_no_nan\n",
    "feature_num_all=[]\n",
    "lr_all_feature=[]\n",
    "svm_all_feature=[]\n",
    "lr_fls_feature=[]\n",
    "svm_fls_feature=[]\n",
    "base_labels= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\stats.py:2248: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (a - mns) / sstd\n"
     ]
    }
   ],
   "source": [
    "#X=X.reshape(X.size,1)\n",
    "#X=X.astype(np.float64,copy=False)\n",
    "np.isnan(X).any()\n",
    "#feature_num=features.shape[1]\n",
    "\n",
    "X[np.isnan(X)] = np.median(X[~np.isnan(X)])\n",
    "clf = ExtraTreesClassifier(n_estimators=30000,\n",
    "                              random_state=0)\n",
    "clf = clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "importances\n",
    "indices = np.argsort(importances)[::-1]\n",
    "#print(\"Feature ranking:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for f in range(X.shape[1]):\n",
    "#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "n_features = [10,30,50,70,80,100,1000,2000,13000,18000,10000,20000,3000,30000,4000,500,5000,6000,7000,8000,9000,15000,25000]\n",
    "n_features = [1000]\n",
    "n_features.sort()\n",
    "\n",
    "for i in n_features:\n",
    "#     #print(i)\n",
    "    \n",
    "#     #lsvc = LinearSVC(C=J[i],penalty=\"l1\", dual=True).fit(X, y)\n",
    "#     #model = SelectFromModel(lsvc, prefit=True)\n",
    "#     #features = model.transform(X)\n",
    "#     clf = ExtraTreesClassifier(n_estimators=250,\n",
    "#                               random_state=0)\n",
    "#     clf = clf.fit(X, y)\n",
    "#     importances = forest.feature_importances_\n",
    "       \n",
    "    index=indices[0:i]\n",
    "    features=X[:,index]\n",
    "#     clf.feature_importances_ \n",
    "\n",
    "#     model = SelectFromModel(clf, threshold=0.6,prefit=True)\n",
    "#     features = model.transform(X)\n",
    "#     features.shape               \n",
    "#     #features=features.reshape(features.size,1)\n",
    "#     #features=features.astype(np.float64,copy=false)\n",
    "#     np.isnan(features).any()\n",
    "    feature_num=features.shape[1]\n",
    "    #print(feature_num)\n",
    "    \n",
    "  \n",
    "    lr_all_accs = []\n",
    "    lr_all_scores = []\n",
    "    lr_f1s = []\n",
    "    svm_all_accs = []\n",
    "    svm_all_scores = []\n",
    "    svm_f1s = []\n",
    "    #base_labels = []\n",
    "    for runs in range(1):\n",
    "        lr_accuracies = []\n",
    "        lr_scores = []\n",
    "        svm_accuracies = []\n",
    "        svm_scores = []\n",
    "        strat_labels = []\n",
    "        \n",
    "        logistic = linear_model.LogisticRegression(C=1e5)\n",
    "        rbf = svm.SVC(C=10,kernel='linear',gamma=0.01)\n",
    "\n",
    "        skf=RepeatedStratifiedKFold(n_splits=5, n_repeats=20)\n",
    "        for train_index, test_index in skf.split(features, y):\n",
    "                    train_data, test_data = features[train_index], features[test_index]\n",
    "                    train_labels, test_labels = y[train_index], y[test_index]\n",
    "\n",
    "                    strat_labels = np.append(strat_labels, test_labels)\n",
    "\n",
    "                    log = logistic.fit(train_data, train_labels)\n",
    "                    log_prob = log.decision_function(test_data)\n",
    "                    log_acc = log.score(test_data, test_labels)\n",
    "\n",
    "                    #f1 calculation\n",
    "                    y_pred = logistic.predict(test_data)\n",
    "                    log_f1 = f1_score(test_labels, y_pred)\n",
    "\n",
    "                    lr_accuracies = np.append(lr_accuracies, log_acc)\n",
    "                    lr_scores = np.append(lr_scores, log_prob)\n",
    "                    lr_f1s = np.append(lr_f1s, log_f1)\n",
    "                    #print('Logistic Regression Accuracy: %f' % log_acc)\n",
    "\n",
    "                    rbf = rbf.fit(train_data, train_labels)\n",
    "                    svm_acc = rbf.score(test_data, test_labels)\n",
    "                    svm_prob = rbf.decision_function(test_data)\n",
    "\n",
    "                    #f1 calculation\n",
    "                    y_pred = rbf.predict(test_data)\n",
    "                    svm_f1 = f1_score(test_labels, y_pred)\n",
    "                    print('SVM Accuracy: %f' % svm_acc)\n",
    "                    svm_accuracies = np.append(svm_accuracies, svm_acc)\n",
    "                    svm_scores = np.append(svm_scores, svm_prob)\n",
    "                    svm_f1s = np.append(svm_f1s, svm_f1)\n",
    "                    print('SVM f1: %f' % svm_f1)\n",
    "\n",
    "\n",
    "                    lr_all_scores = np.append(lr_all_scores, lr_scores)\n",
    "                    lr_fold_avg = np.mean(lr_accuracies)\n",
    "                    lr_all_accs = np.append(lr_all_accs, lr_fold_avg)\n",
    "                    svm_all_scores = np.append(svm_all_scores, svm_scores)\n",
    "                    svm_fold_avg = np.mean(svm_accuracies)\n",
    "                    svm_all_accs = np.append(svm_all_accs, svm_fold_avg)\n",
    "                    #print('Logistic Regression Accuracy: %f' % log_acc_avg)\n",
    "                            #print('SVM Regression Accuracy: %f' % svm_acc_avg)\n",
    "        feature_num_all=np.append(feature_num_all,feature_num)\n",
    "# print(np.mean(lr_all_accs))\n",
    "# print(np.mean(svm_all_accs))      \n",
    "        lr_all_feature=np.append(lr_all_feature,np.mean(lr_all_accs))\n",
    "        svm_all_feature=np.append(svm_all_feature,np.mean(svm_all_accs))\n",
    "        lr_fls_feature=np.append(lr_fls_feature,np.mean(lr_f1s))\n",
    "        svm_fls_feature=np.append(svm_fls_feature,np.mean(svm_f1s))\n",
    "        #base_labels_all = np.append(base_labels_all,strat)\n",
    "        base_labels = np.append(base_labels, np.mean(strat_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( (feature_num_all), lr_all_feature*100, lw=5, label='Logistic Regression')\n",
    "plt.plot( (feature_num_all), svm_all_feature*100, lw=5, label='SVM')\n",
    "plt.xlim([50, 30000])\n",
    "plt.ylim([0, 100])\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MCIvsSMI')\n",
    "plt.legend(loc=\"lower right\")\n",
    "#plt.savefig('/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMI_ALL.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"svm accuracy: {}\".format(np.mean(svm_accuracies)))\n",
    "print(\"svm accuracy, sd: {}\".format(np.std(svm_accuracies)))\n",
    "\n",
    "print(\"svm f1: {}\".format(np.mean(svm_f1s)))\n",
    "#print(svm_f1s)\n",
    "#print(svm_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(svm_f1s)\n",
    "\n",
    "# print(\"LR accuracy Avg: {}\".format(np.mean(lr_all_feature)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "# print(\"LR accuracy Standard Deviation: {}\".format(np.std(lr_all_feature)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "# print(\"LR f1s Avg : {}\".format(np.mean( lr_fls_feature)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "# print(\"LR f1s Standard Deviation: {}\".format(np.std(lr_f1s)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "# print(\"SVM Avg: {}\".format(np.mean(svm_all_feature)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "# print(\"SVM Standard Deviation: {}\".format(np.std(svm_all_accs)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "# print(\"SVM f1s Avg : {}\".format(np.mean( svm_fls_feature )),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "# print(\"SVM f1s Standard Deviation: {}\".format(np.std(svm_fls_feature)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "lrindex=np.argmax(lr_all_feature)\n",
    "# print(\"LR Maximum feature number is: {}\".format(feature_num_all[lrindex]),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "svmindex=np.argmax(svm_all_feature)\n",
    "# print(\"SVM Maximum feature is number : {}\".format(feature_num_all[svmindex]),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "clf = clf.fit(X, y)\n",
    "importances = clf.feature_importances_\n",
    "#importances\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(100):\n",
    "    print(\"%d. feature %d %s (%f)\" % (f + 1, indices[f],headers[indices[f]],importances[indices[f]]),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMI_rank.txt\", \"a\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\stats\\stats.py:2248: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (a - mns) / sstd\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-97ab85fd68e4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m#Feature selection for train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                 \u001b[0mimportances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportances\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    245\u001b[0m         \"\"\"\n\u001b[0;32m    246\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 453\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\juke\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 44\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "#################################################################################################################    \n",
    "from sklearn import metrics\n",
    "\n",
    "#max_features = feature_num_all[svmindex]\n",
    "#print(max_features)\n",
    "lr_all_accs = []\n",
    "lr_all_scores = []\n",
    "lr_f1s = []\n",
    "svm_all_accs = []\n",
    "svm_all_scores = []\n",
    "svm_f1s = []\n",
    "base_labels = []\n",
    "lr_sensitivity= []\n",
    "svm_sensitivity=[]\n",
    "lr_specificity=[]\n",
    "svm_specificity=[]\n",
    "\n",
    "#index=indices[0:int(max_features)]\n",
    "#features=X[:,index]\n",
    "#feature_num=features.shape[1]\n",
    "for runs in range(1):\n",
    "    lr_accuracies = []\n",
    "    lr_scores = []\n",
    "    svm_accuracies = []\n",
    "    svm_scores = []\n",
    "    strat_labels = []\n",
    "\n",
    "    logistic = linear_model.LogisticRegression(C=1e5)\n",
    "    rbf = svm.SVC(C=10,kernel='linear',gamma=0.01)\n",
    "    \n",
    "    skf=RepeatedStratifiedKFold(n_splits=10, n_repeats=10)\n",
    "    \n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "                \n",
    "                #####feature selection within cv###############################\n",
    "                clf = ExtraTreesClassifier(n_estimators=30000,random_state=0)\n",
    "                X=data[:,idx_IN_columns]\n",
    "                #features=data[:,11:data.shape[1]]\n",
    "                #features = features.transpose()\n",
    "                X = stats.zscore(X)\n",
    "                #print(features.shape)\n",
    "                y=data[:,6]\n",
    "                #5: ad-smi / 6:mci-smi / 7:adonly-smi / 8:ad-mci / 9:adonly-mci / 10:adonly - adwithsmallvv\n",
    "                ind_num=np.isnan(y)\n",
    "                # print(ind_num.shape)\n",
    "\n",
    "                y_no_nan = y[~ind_num]\n",
    "\n",
    "                X_no_nan = X[~ind_num,:]\n",
    "\n",
    "               # print(y.shape)\n",
    "\n",
    "                y=y_no_nan\n",
    "                X=X_no_nan\n",
    "                \n",
    "                train_X,test_X=X[train_index,:],X[test_index,:] \n",
    "                train_labels, test_labels = y[train_index], y[test_index]\n",
    "                \n",
    "                #Feature selection for train set\n",
    "                clf = clf.fit(train_X, train_labels)\n",
    "                importances = clf.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                index=indices[0:999]\n",
    "                train_data=train_X[:,index]\n",
    "                \n",
    "                #Feature selection for test set\n",
    "                clf = clf.fit(test_X, test_labels)\n",
    "                importances = clf.feature_importances_\n",
    "                indices = np.argsort(importances)[::-1]\n",
    "                index=indices[0:999]\n",
    "                test_data=train_X[:,index]\n",
    "                #################################################################\n",
    "                \n",
    "                #train_data, test_data = X[train_index], X[test_index]\n",
    "                #train_labels, test_labels = y[train_index], y[test_index]\n",
    "                \n",
    "                \n",
    "                #print(train_data.shape)\n",
    "                strat_labels = np.append(strat_labels, test_labels)\n",
    "\n",
    "                log = logistic.fit(train_data, train_labels)\n",
    "                log_prob = log.decision_function(test_data)\n",
    "                log_acc = log.score(test_data, test_labels)\n",
    "\n",
    "                #f1 calculation\n",
    "                y_pred = logistic.predict(test_data)\n",
    "                log_f1 = f1_score(test_labels, y_pred)\n",
    "                log_confuse= metrics.confusion_matrix(test_labels, y_pred)\n",
    "                TP = log_confuse[1, 1]\n",
    "                TN = log_confuse[0, 0]\n",
    "                FP = log_confuse[0, 1]\n",
    "                FN = log_confuse[1, 0]\n",
    "                log_sen=metrics.recall_score(test_labels, y_pred)\n",
    "    #             print(TN)\n",
    "    #             print(FP)\n",
    "                log_spec=TN / (TN + FP)\n",
    "    #             print(log_confuse)\n",
    "    #             print(log_sen)\n",
    "    #             print(log_spec)\n",
    "\n",
    "                lr_accuracies = np.append(lr_accuracies, log_acc)\n",
    "                lr_sensitivity=np.append(lr_sensitivity, log_sen)\n",
    "                lr_specificity=np.append(lr_specificity, log_spec)\n",
    "\n",
    "                lr_scores = np.append(lr_scores, log_prob)\n",
    "                lr_f1s = np.append(lr_f1s, log_f1)\n",
    "\n",
    "                #print('Logistic Regression Accuracy: %f' % log_acc)\n",
    "\n",
    "                rbf = rbf.fit(train_data, train_labels)\n",
    "                svm_acc = rbf.score(test_data, test_labels)\n",
    "                svm_prob = rbf.decision_function(test_data)\n",
    "\n",
    "                #f1 calculation\n",
    "                y_pred = rbf.predict(test_data)\n",
    "                svm_f1 = f1_score(test_labels, y_pred)\n",
    "                svm_confuse= metrics.confusion_matrix(test_labels, y_pred)\n",
    "                TP = log_confuse[1, 1]\n",
    "                TN = log_confuse[0, 0]\n",
    "                FP = log_confuse[0, 1]\n",
    "                FN = log_confuse[1, 0]\n",
    "                svm_sen=metrics.recall_score(test_labels, y_pred)\n",
    "                svm_spec=TN / (TN + FP)\n",
    "\n",
    "\n",
    "\n",
    "                svm_accuracies = np.append(svm_accuracies, log_acc)\n",
    "                svm_sensitivity=np.append(svm_sensitivity, log_sen)\n",
    "                svm_specificity=np.append(svm_specificity, log_spec)\n",
    "\n",
    "                #print('SVM Accuracy: %f' % svm_acc)\n",
    "                svm_accuracies = np.append(svm_accuracies, svm_acc)\n",
    "                svm_scores = np.append(svm_scores, svm_prob)\n",
    "                svm_f1s = np.append(svm_f1s, svm_f1)\n",
    "\n",
    "    base_labels = np.append(base_labels, strat_labels)\n",
    "    lr_all_scores = np.append(lr_all_scores, lr_scores)\n",
    "    lr_fold_avg = np.mean(lr_accuracies)\n",
    "    lr_all_accs = np.append(lr_all_accs, lr_fold_avg)\n",
    "    svm_all_scores = np.append(svm_all_scores, svm_scores)\n",
    "    svm_fold_avg = np.mean(svm_accuracies)\n",
    "    svm_all_accs = np.append(svm_all_accs, svm_fold_avg)\n",
    "\n",
    "# feature_num_all=np.append(feature_num_all,feature_num)\n",
    "# # print(np.mean(lr_all_accs))\n",
    "print(np.mean(svm_all_accs))      \n",
    "# lr_all_feature=np.append(lr_all_feature,np.mean(lr_all_accs))\n",
    "# print(lr_all_feature)\n",
    "# svm_all_feature=np.append(svm_all_feature,np.mean(svm_all_accs))\n",
    "# print(np.mean(svm_accuracies))\n",
    "# lr_fls_feature=np.append(lr_fls_feature,np.mean(lr_f1s))\n",
    "# svm_fls_feature=np.append(svm_fls_feature,np.mean(svm_f1s))\n",
    "#base_labels_all = np.append(base_labels_all,strat)\n",
    "#base_labels = np.append(base_labels, np.mean(strat_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"LR accuracy Avg: {}\".format(np.mean(lr_accuracies)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR accuracy Standard Deviation: {}\".format(np.std(lr_accuracies)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR sensitivity Avg: {}\".format(np.mean(lr_sensitivity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR sensitivity Standard Deviation: {}\".format(np.std(lr_sensitivity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR specificity Avg: {}\".format(np.mean(lr_specificity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR specificity Standard Deviation: {}\".format(np.std(lr_specificity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR f1s Avg : {}\".format(np.mean(lr_f1s)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"LR f1s Standard Deviation: {}\".format(np.std(lr_f1s)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "print(\"SVM Avg: {}\".format(np.mean(svm_accuracies)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM Standard Deviation: {}\".format(np.std(svm_accuracies)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM sensitivity Avg: {}\".format(np.mean(svm_sensitivity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM sensitivity Standard Deviation: {}\".format(np.std(svm_sensitivity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM specificity Avg: {}\".format(np.mean(svm_specificity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM specificity Standard Deviation: {}\".format(np.std(svm_specificity)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM f1s Avg : {}\".format(np.mean(svm_f1s)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "print(\"SVM f1s Standard Deviation: {}\".format(np.std(svm_f1s)),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "#lrindex=np.argmax(lr_all_feature)\n",
    "print(\"LR Maximum feature number is: {}\".format(feature_num_all[lrindex]),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "#svmindex=np.argmax(svm_all_feature)\n",
    "print(\"SVM Maximum feature is number : {}\".format(feature_num_all[svmindex]),file=open(\"/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/MCIvsSMIaccuracy.txt\", \"a\"))\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc,f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "fpr['lr'], tpr['lr'], _ = roc_curve(base_labels, lr_all_scores)\n",
    "roc_auc['lr'] = auc(fpr['lr'], tpr['lr'])\n",
    "#f1['lr']=f1_score(fpr['lr'], tpr['lr'])\n",
    "fpr['svm'], tpr['svm'], _ = roc_curve(base_labels, svm_all_scores)\n",
    "roc_auc['svm'] = auc(fpr['svm'], tpr['svm'])\n",
    "#f1['svm']=f1_score(fpr['svm'], tpr['svm'])\n",
    "#fpr['gcn'], tpr['gcn'], _ = roc_curve(all_labels, all_scores)\n",
    "#roc_auc['gcn'] = auc(fpr['gcn'], tpr['gcn'])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr['lr'], tpr['lr'], lw=5, label='Logistic Regression (area = %0.2f)' % roc_auc['lr'] )\n",
    "#plt.plot(fpr['lr'], tpr['lr'], lw=3, label='Logistic Regression (f1 = %0.2f)' % f1['lr'] )\n",
    "plt.plot(fpr['svm'], tpr['svm'], lw=5, label='SVM (area = %0.2f)' % roc_auc['svm'] )\n",
    "#plt.plot(fpr['svm'], tpr['svm'f], lw=3, label='SVM (f1 = %0.2f)' % f1['svm'] )\n",
    "#plt.plot(fpr['gcn'], tpr['gcn'], lw=3, label='GCN (area = %0.2f)' % roc_auc['gcn'])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=5, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('MCIvsSMI')\n",
    "plt.legend(loc=\"lower right\") \n",
    "#plt.savefig('10x_Combined_ROC.eps')\n",
    "#plt.savefig('ROC_MCIVsNormal_connectome.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.savefig('/Users/yunwang/Dropbox (NYSPI)/00 AD ML/0papers/0Neuroimage-Clinical/Figures/ROC_MCIvsSMI_connectome.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
