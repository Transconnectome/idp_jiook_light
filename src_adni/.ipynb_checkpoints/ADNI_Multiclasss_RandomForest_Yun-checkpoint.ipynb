{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished this block\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from sklearn import linear_model, svm\n",
    "import re\n",
    "from sklearn.metrics import roc_curve, auc,f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#s = \"../braindata/data_1_mor_select_100.csv\"\n",
    "import os          \n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "import os\n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import *\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,cross_val_predict,StratifiedKFold,KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix, mean_squared_error,precision_score,recall_score,f1_score,hamming_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('finished this block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished this block\n"
     ]
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir('../data/adni')\n",
    "print('finished this block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the original training data dimension is\n",
      "(179, 34733)\n"
     ]
    }
   ],
   "source": [
    "dd =pd.read_csv(\"combine_new_biomarker_correct.csv\",header=0)\n",
    "print('the original training data dimension is')\n",
    "print(dd.shape)\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=list(dd)\n",
    "start_index_conn=names.index('T1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after the precoessing, the X.shape is \n",
      "(179, 34709)\n",
      "the y.shape is\n",
      "(179,)\n"
     ]
    }
   ],
   "source": [
    "with open('combine_new_biomarker.csv', 'r') as f:\n",
    "    d_reader = csv.DictReader(f)\n",
    "    #get fieldnames from DictReader object and store in list\n",
    "    headers = d_reader.fieldnames\n",
    "    \n",
    "data=np.array(dd)\n",
    "#idx_IN_columns = np.append(np.array([9,10]),np.array(range(start_index_conn,data.shape[1])))\n",
    "\n",
    "#print(idx_IN_columns)\n",
    "\n",
    "X_conn=data[:,start_index_conn:]  #Connectome features\n",
    "X_biomarker=data[:,1:6] # Biomarker features\n",
    "y_baseline=data[:,10]  #baseline 1 is normal  2 MCI  3 is AD\n",
    "y_final=data[:,9]   #Finaldx # 0 is normal 1 is AD \n",
    "\n",
    "\n",
    "#print(y_final)\n",
    "#print(y_baseline)\n",
    "\n",
    "\n",
    "y=np.zeros(y_baseline.shape)\n",
    "#print(len(y))\n",
    "\n",
    "for i in range(0,len(y)):\n",
    "    if y_baseline[i]==1:\n",
    "        if y_final[i]==1:\n",
    "            y[i]=2\n",
    "        else:\n",
    "            y[i]=0\n",
    "    elif y_baseline[i]==2:\n",
    "        if y_final[i]==0:\n",
    "            y[i]=1\n",
    "        else:\n",
    "            y[i]=2\n",
    "    else: \n",
    "        y[i]=3\n",
    "\n",
    "        \n",
    "        \n",
    "# X_marker=X[:,2:]\n",
    "\n",
    "ind_num_matrix=np.isnan(X_conn)\n",
    "ind_num_vector=np.any(ind_num_matrix,axis=1)\n",
    "\n",
    "X_no_nan=X_conn[~ind_num_vector,:]\n",
    "y_no_nan=y[~ind_num_vector]\n",
    "\n",
    "X_conn_final=X_no_nan\n",
    "y=y_no_nan        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "feature_num_all=[]\n",
    "\n",
    "lr_all_feature=[]\n",
    "svm_all_feature=[]\n",
    "XGreg_all_feature=[]\n",
    "\n",
    "\n",
    "lr_fls_feature=[]\n",
    "svm_fls_feature=[]\n",
    "XGB_f1s_feature=[]\n",
    "\n",
    "base_labels= []\n",
    "# np.isnan(X).any()\n",
    "\n",
    "# np.isnan(X).any()\n",
    "#X[np.isnan(X_)] = np.median(X[~np.isnan(X)])\n",
    "\n",
    "print(\"after the precoessing, the X.shape is \")\n",
    "print(X_conn_final.shape)\n",
    "print(\"the y.shape is\")\n",
    "print(y.shape)\n",
    "\n",
    "X=X_conn_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler()\n",
    "X_scaler=scaler.fit_transform(X.astype(np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "rs=33994\n",
    "outer_cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=rs)\n",
    "for train_index, test_index in outer_cv.split(X_scaler, y):\n",
    "    X_train, X_test = X_scaler[train_index], X_scaler[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    ovo_clf=OneVsOneClassifier(ExtraTreesClassifier(n_estimators=250,random_state=42))\n",
    "    ovo_clf.fit(X_train,y_train)\n",
    "    ovo_clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57142857, 0.42857143, 0.46153846, 0.38461538, 0.61538462,\n",
       "       0.53846154, 0.6       , 0.7       , 0.5       , 0.5       ])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score( ovo_clf,X_train,y_train,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred=cross_val_predict(ovo_clf,X_train,y_train,cv=10)\n",
    "conf_mx=confusion_matrix(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_sums=conf_mx.sum(axis=1,keepdims=True)\n",
    "norm_conf_mx=conf_mx/row_sums\n",
    "np.fill_diagonal(norm_conf_mx,0)\n",
    "plt.matshow(norm_conf_mx,cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 3\n",
    "\n",
    "all_TP = []\n",
    "all_TN = []\n",
    "all_FP = []\n",
    "all_FN = []\n",
    "\n",
    "all_acc = []\n",
    "all_sen = []\n",
    "all_spec = []\n",
    "all_auc = []\n",
    "\n",
    "all_roc_label = []\n",
    "all_roc_pred = []\n",
    "all_roc_prob = []\n",
    "\n",
    "#rs_list=[33994,31358,27381,8642,7012,42023,44642,44002,30706,12571]\n",
    "rs_list=[33994]\n",
    "for rs in rs_list:\n",
    "        print('********random seed:{}'.format(rs))\n",
    "\n",
    "        inner_cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=rs)\n",
    "        outer_cv = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=rs)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        #scaler = RobustScaler()\n",
    "        scaler.fit(X)\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        avg_acc = []\n",
    "        avg_TP = []\n",
    "        avg_TN = []\n",
    "        avg_FP = []\n",
    "        avg_FN = []\n",
    "        avg_sen = []\n",
    "        avg_spec = []\n",
    "        avg_auc = []\n",
    "\n",
    "\n",
    "        roc_label = []\n",
    "        roc_pred = []\n",
    "        roc_prob = []\n",
    "        \n",
    "        for train_index, test_index in outer_cv.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            # 'featureExtract__n_estimators': np.arange(10, 100, 10),\n",
    "            \n",
    "            #grid1######################################\n",
    "            #params = {'randomforest__min_samples_leaf': np.arange(1, 51, 5),\n",
    "            #          'randomforest__n_estimators': np.arange(10, 100, 10)}\n",
    "            \n",
    "            #grid2######################################\n",
    "            #params = {'randomforest__min_samples_leaf': np.arange(1, 51, 1),\n",
    "            #          'randomforest__n_estimators': np.arange(10, 500, 10)}\n",
    "            \n",
    "            #grid3######################################\n",
    "            params = {'randomforest__min_samples_leaf': np.arange(1, 51, 5),\n",
    "                      'randomforest__n_estimators': np.arange(10, 500, 10)}\n",
    "            \n",
    "#             params_svm = [\n",
    "#                     {'randomforest__min_samples_leaf': np.arange(1, 51, 5),\n",
    "#                       'randomforest__n_estimators': np.arange(10, 100, 10)},\n",
    "#                     {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000]},\n",
    "#                     {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}                \n",
    "#             ]\n",
    "            \n",
    "            #pipe = Pipeline([\n",
    "            #    ('featureExtract', SelectFromModel(ExtraTreesClassifier())),\n",
    "            #    ('randomforest', RandomForestClassifier())\n",
    "            #])\n",
    "            \n",
    "            pipe_rf = Pipeline([\n",
    "                ('featureExtract', SelectFromModel(ExtraTreesClassifier())),\n",
    "                ('randomforest', RandomForestClassifier())\n",
    "            ])\n",
    "\n",
    "            \n",
    "            \n",
    "            #clf = GridSearchCV(estimator=pipe, param_grid=params, cv=inner_cv, scoring='accuracy',n_jobs=8)\n",
    "            clf = GridSearchCV(estimator=pipe_rf, param_grid=params, cv=inner_cv,scoring='accuracy', n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sep1 = '*' * 100\n",
    "sep2 = '*' * 50\n",
    "sep3 = '*' * 30\n",
    "\n",
    "print(\"beginning selection best number of features\")\n",
    "n_features = [10,30,50,70,80,100,1000,2000]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rbf = svm.SVC(C=10,kernel='rbf',gamma=0.01)\n",
    "\n",
    "for i in n_features:\n",
    "    print(\"\\n\\n Number of Feature: {} {} \\n\".format(i, sep1))\n",
    "      \n",
    "    SVM_all_accs_one_feature  = []\n",
    "    SVM_all_f1s_one_feature = []\n",
    "    base_labels=[]\n",
    "    \n",
    "    for runs in range(1):\n",
    "        counter=0\n",
    "        print(\"\\n RUN: {} {} \\n\".format(runs, sep3))\n",
    "        \n",
    "        SVM_accuracies = []\n",
    "        SVM_f1s=[] \n",
    "        strat_labels = []\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=int(runs))\n",
    "        for train_index, test_index in skf.split(X, y):       \n",
    "            print(\"\\n Fold: {} {} \\n\".format(counter, sep2))        \n",
    "            counter=counter+1\n",
    "                        \n",
    "            train_data_origin, test_data_origin=X[train_index], X[test_index]\n",
    "            train_labels, test_labels = y[train_index], y[test_index]\n",
    "            strat_labels = np.append(strat_labels, test_labels)\n",
    "\n",
    " #           print(\"Random forest for feature selection\")\n",
    "            clf = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "            clf = clf.fit(train_data_origin, train_labels)\n",
    "            importances = clf.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]        \n",
    "            \n",
    "            index=indices[0:i]\n",
    "            train_data=train_data_origin[:, index]\n",
    "            test_data=test_data_origin[:, index]\n",
    "                                     \n",
    "        #SVM\n",
    "            rbf = rbf.fit(train_data, train_labels)\n",
    "            svm_acc = rbf.score(test_data, test_labels)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #f1 calculation\n",
    "            y_pred = rbf.predict(test_data)\n",
    "            \n",
    "            print(y_pred)\n",
    "            print(test_labels)\n",
    "            \n",
    "            svm_f1 = f1_score(test_labels, y_pred, average='macro')\n",
    "            \n",
    "                \n",
    "            print('SVM Accuracy: %f' % svm_acc)\n",
    "            print('SVM F1 score: %f' % svm_f1)\n",
    "                    \n",
    "            SVM_accuracies = np.append(SVM_accuracies, svm_acc)\n",
    "            SVM_f1s=np.append(SVM_f1s, svm_f1)             \n",
    "                            \n",
    "        base_labels=np.append(base_labels, strat_labels)        \n",
    "        SVM_all_accs_one_feature=np.append(SVM_all_accs_one_feature, np.mean(SVM_accuracies))\n",
    "        #SVM_all_f1s_one_feature =np.append(SVM_all_f1s_one_feature, np.mean(SVM_f1s)) \n",
    "\n",
    "    print(\"SVM###############################################################\")\n",
    "    print(\"Runs Avg Accuracies: {}\".format(np.mean(SVM_accuracies)))\n",
    "    print(\"Standard Deviation: {}\".format(np.std(SVM_accuracies)))\n",
    "    #print(\"Runs Avg F1: {}\".format(np.mean(SVM_f1s)))\n",
    "    #print(\"Standard Deviation: {}\".format(np.std(SVM_f1s)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('begin logistic regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep1 = '*' * 100\n",
    "sep2 = '*' * 50\n",
    "sep3 = '*' * 30\n",
    "\n",
    "print(\"beginning selection best number of features\")\n",
    "n_features = [10,30,50,70,80,100,1000,2000]\n",
    "\n",
    "logistic = linear_model.LogisticRegression(C=1e5)\n",
    "\n",
    "for i in n_features:\n",
    "    print(\"\\n\\n Number of Feature: {} {} \\n\".format(i, sep1))\n",
    "      \n",
    "    LR_all_accs_one_feature  = []\n",
    "    LR_all_f1s_one_feature = []\n",
    "\n",
    "\n",
    "    base_labels=[]\n",
    "    \n",
    "    for runs in range(1):\n",
    "        counter=0\n",
    "        print(\"\\n RUN: {} {} \\n\".format(runs, sep3))\n",
    "        \n",
    "        LR_accuracies = []\n",
    "        LR_f1s=[]    \n",
    "    \n",
    "\n",
    "        strat_labels = []\n",
    "        \n",
    "        logistic = linear_model.LogisticRegression(C=1e4)\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=int(runs))\n",
    "        for train_index, test_index in skf.split(X, y):       \n",
    "            print(\"\\n Fold: {} {} \\n\".format(counter, sep2))        \n",
    "            counter=counter+1\n",
    "                        \n",
    "            train_data_origin, test_data_origin=X[train_index], X[test_index]\n",
    "            train_labels, test_labels = y[train_index], y[test_index]\n",
    "            strat_labels = np.append(strat_labels, test_labels)\n",
    "\n",
    " #           print(\"Random forest for feature selection\")\n",
    "            clf = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "            clf = clf.fit(train_data_origin, train_labels)\n",
    "            importances = clf.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]        \n",
    "            \n",
    "            index=indices[0:i]\n",
    "            train_data=train_data_origin[:, index]\n",
    "            test_data=test_data_origin[:, index]\n",
    "                             \n",
    "            #Logistic Regression\n",
    "            log = logistic.fit(train_data, train_labels)\n",
    "            log_acc = log.score(test_data, test_labels)\n",
    "\n",
    "            #f1 calculation            \n",
    "            y_pred = logistic.predict(test_data)            \n",
    "            log_f1 = f1_score(test_labels, y_pred, average='macro')\n",
    "\n",
    "            print('Logistic Regression Accuracy: %f' % log_acc)\n",
    "            print('Logistic Regression F1 score: %f' % log_f1)\n",
    "                            \n",
    "            LR_accuracies = np.append(LR_accuracies, log_acc)\n",
    "            LR_f1s=np.append(LR_f1s, log_f1)        \n",
    "                      \n",
    "        base_labels=np.append(base_labels, strat_labels)        \n",
    "        LR_all_accs_one_feature=np.append(LR_all_accs_one_feature, np.mean(LR_accuracies))\n",
    "        LR_all_f1s_one_feature =np.append(LR_all_f1s_one_feature, np.mean(LR_f1s))    \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"LR###############################################################\")\n",
    "    print(\"Runs Avg Accuracies: {}\".format(np.mean(LR_accuracies)))\n",
    "    print(\"Standard Deviation: {}\".format(np.std(LR_accuracies)))\n",
    "    print(\"Runs Avg F1: {}\".format(np.mean(LR_f1s)))\n",
    "    print(\"Standard Deviation: {}\".format(np.std(LR_f1s)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('begin XGboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep1 = '*' * 100\n",
    "sep2 = '*' * 50\n",
    "sep3 = '*' * 30\n",
    "\n",
    "print(\"beginning selection best number of features\")\n",
    "n_features = [10,30,50,70,80,100,1000,2000]\n",
    "\n",
    "\n",
    "param={}\n",
    "param['objective'] = 'multi:softmax'\n",
    "\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = 6\n",
    "\n",
    "for i in n_features:\n",
    "    print(\"\\n\\n Number of Feature: {} {} \\n\".format(i, sep1))\n",
    "      \n",
    "    XGB_all_accs_one_feature  = []\n",
    "    XGB_all_f1s_one_feature = []\n",
    "\n",
    "\n",
    "    base_labels=[]\n",
    "    \n",
    "    for runs in range(10):\n",
    "        counter=0\n",
    "        print(\"\\n RUN: {} {} \\n\".format(runs, sep3))\n",
    "        \n",
    "        XGB_accuracies = []\n",
    "        XGB_f1s=[]    \n",
    "    \n",
    "\n",
    "        strat_labels = []\n",
    "        \n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=int(runs))\n",
    "        for train_index, test_index in skf.split(X, y):       \n",
    "            print(\"\\n Fold: {} {} \\n\".format(counter, sep2))        \n",
    "            counter=counter+1\n",
    "                        \n",
    "            train_data_origin, test_data_origin=X[train_index], X[test_index]\n",
    "            train_labels, test_labels = y[train_index], y[test_index]\n",
    "            strat_labels = np.append(strat_labels, test_labels)\n",
    "\n",
    " #           print(\"Random forest for feature selection\")\n",
    "            clf = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "            clf = clf.fit(train_data_origin, train_labels)\n",
    "            importances = clf.feature_importances_\n",
    "            indices = np.argsort(importances)[::-1]        \n",
    "            \n",
    "            index=indices[0:i]\n",
    "            train_data=train_data_origin[:, index]\n",
    "            test_data=test_data_origin[:, index]\n",
    "            \n",
    "            xgb_train_data=xgb.DMatrix(train_data, label=train_labels)\n",
    "            xgb_test_data=xgb.DMatrix(test_data, label=test_labels)\n",
    "            \n",
    "                             \n",
    "            #Logistic Regression\n",
    "            watchlist=[(xgb_train_data, 'train'), (xgb_test_data, 'test')]\n",
    "            num_round=5\n",
    "            bst = xgb.train(param, xgb_train_data, num_round, watchlist)\n",
    "            y_pred = bst.predict(xgb_test_data)\n",
    "\n",
    "            print(y_pred)\n",
    "            \n",
    "            print(test_labels)\n",
    "            \n",
    "            print(np.sum(y_pred == test_labels))\n",
    "            \n",
    "            print(test_labels[0])\n",
    "            \n",
    "            xgb_acc = np.sum(y_pred == test_labels) / test_labels.shape[0]           \n",
    "\n",
    "            #f1 calculation\n",
    "            xgb_f1 = f1_score(test_labels, y_pred, average='macro')\n",
    "\n",
    "            print('XGB Regression Accuracy: %f' % xgb_acc)\n",
    "            print('XGB Regression F1 score: %f' % xgb_f1)\n",
    "                            \n",
    "            XGB_accuracies = np.append(LR_accuracies, log_acc)\n",
    "            XGB_f1s=np.append(LR_f1s, log_f1)        \n",
    "               \n",
    "        base_labels=np.append(base_labels, strat_labels)        \n",
    "        XGB_all_accs_one_feature=np.append(LR_all_accs_one_feature, np.mean(LR_accuracies))\n",
    "        XG_all_f1s_one_feature =np.append(LR_all_f1s_one_feature, np.mean(LR_f1s))    \n",
    "\n",
    "    #print(\"LR###############################################################\")\n",
    "    #print(\"Runs Avg Accuracies: {}\".format(np.mean(LR_accuracies)))\n",
    "    #print(\"Standard Deviation: {}\".format(np.std(LR_accuracies)))\n",
    "    #print(\"Runs Avg F1: {}\".format(np.mean(LR_f1s)))\n",
    "    #print(\"Standard Deviation: {}\".format(np.std(LR_f1s)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
