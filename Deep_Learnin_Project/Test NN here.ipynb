{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import modules\n",
    "import os \n",
    "import scipy.io\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 211)\n",
      "(84, 84, 211)\n",
      "(164, 164, 211)\n",
      "(164, 164, 211)\n"
     ]
    }
   ],
   "source": [
    "test=scipy.io.loadmat('idp_connectome_aparc_length.mat')\n",
    "aparcl=np.array(test['connectome_aparc_length'])\n",
    "print(aparcl.shape)\n",
    "\n",
    "test=scipy.io.loadmat('idp_connectome_aparc_count.mat')\n",
    "aparcc=np.array(test['connectome_aparc_count'])\n",
    "print(aparcc.shape)\n",
    "\n",
    "test=scipy.io.loadmat('idp_connectome_aparc2009_length.mat')\n",
    "aparc2l=np.array(test['connectome_aparc2009_length'])\n",
    "print(aparc2l.shape)\n",
    "\n",
    "test=scipy.io.loadmat('idp_connectome_aparc2009_count.mat')\n",
    "aparc2c=np.array(test['connectome_aparc2009_count'])\n",
    "print(aparc2c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211, 164, 164, 4)\n"
     ]
    }
   ],
   "source": [
    "zeromatal=np.zeros([164,164,211])\n",
    "zeromatac=np.zeros([164,164,211])\n",
    "\n",
    "zeromatal[40:124,40:124,:]=aparcl\n",
    "zeromatac[40:124,40:124,:]=aparcc\n",
    "\n",
    "aparcl=zeromatal\n",
    "aparcc=zeromatac\n",
    "\n",
    "X=np.zeros([164,164,211,4])\n",
    "X[:,:,:,0]=aparcl\n",
    "X[:,:,:,1]=aparcc\n",
    "X[:,:,:,2]=aparc2l\n",
    "X[:,:,:,3]=aparc2c\n",
    "\n",
    "X=X.transpose([2,0,1,3])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 211 but corresponding boolean dimension is 208",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-a6e032ae14fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mind_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mad_smi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0my_adsmi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mad_smi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mind_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mX_adsmi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mind_num\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mind_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmci_smi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 211 but corresponding boolean dimension is 208"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('idp_data_1_mor.csv',header=0)\n",
    "data=np.array(data)\n",
    "\n",
    "ad_smi=data[:,5]\n",
    "mci_smi=data[:,6]\n",
    "ad_mci=data[:,7]\n",
    "\n",
    "adonly_smi=data[:,8]\n",
    "adonly_mci=data[:,9]\n",
    "adonly_adwithsmallvv=data[:,10]\n",
    "\n",
    "print(ad_smi.shape)\n",
    "\n",
    "ind_num=np.isnan(ad_smi)\n",
    "y_adsmi=ad_smi[~ind_num]\n",
    "X_adsmi=X[~ind_num,:,:,:]\n",
    "\n",
    "ind_num=np.isnan(mci_smi)\n",
    "y_mcismi=mci_smi[~ind_num]\n",
    "X_mcismi=X[~ind_num,:,:,:]\n",
    "\n",
    "ind_num=np.isnan(ad_mci)\n",
    "y_admci=ad_mci[~ind_num]\n",
    "X_admci=X[~ind_num,:,:,:]\n",
    "\n",
    "\n",
    "\n",
    "ind_num=np.isnan(adonly_smi)\n",
    "y_adonlysmi=adonly_smi[~ind_num]\n",
    "X_adonlysmi=X[~ind_num,:,:,:]\n",
    "\n",
    "ind_num=np.isnan(adonly_mci)\n",
    "y_adonlymci=adonly_mci[~ind_num]\n",
    "X_adonlymci=X[~ind_num,:,:,:]\n",
    "\n",
    "ind_num=np.isnan(adonly_adwithsmallvv)\n",
    "y_adonlyadwithsmallvv=adonly_adwithsmallvv[~ind_num]\n",
    "X_adonlyadwithsmallvv=X[~ind_num,:,:,:]\n",
    "\n",
    "\n",
    "    \n",
    "    #skf=RepeatedStratifiedKFold(n_splits=cv, n_repeats=re)\n",
    "    #skf.split(features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from CNN_codes.CNN_test import cnn_training\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the raw CIFAR-10 data.\n",
    "X_train, y_train = load_data(mode='train')\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "num_training = 49000\n",
    "num_validation = 1000\n",
    "\n",
    "X_val = X_train[-num_validation:, :]\n",
    "y_val = y_train[-num_validation:]\n",
    "\n",
    "X_train = X_train[:num_training, :]\n",
    "y_train = y_train[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data, and reshape it to be RGB size\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "X_train = X_train.reshape([-1,32,32,3])/255\n",
    "X_val = X_val.reshape([-1,32,32,3])/255\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (75, 7993)\n",
      "Train labels shape:  (75,)\n",
      "Validation data shape:  (23, 7993)\n",
      "Validation labels shape:  (23,)\n",
      "[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=features\n",
    "y=y\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "num_training = 75\n",
    "num_validation = 23\n",
    "\n",
    "X_val = X[-num_validation:, :]\n",
    "y_val = y[-num_validation:]\n",
    "\n",
    "X_train = X[:num_training, :]\n",
    "y_train = y[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data, and reshape it to be RGB size\n",
    "#mean_image = np.mean(X_train, axis=0)\n",
    "#X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "#X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "#X_train = X_train.reshape([-1,32,32,3])/255\n",
    "#X_val = X_val.reshape([-1,32,32,3])/255\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "(23,)\n",
      "0.6956521739130435\n",
      "0.7391304347826086\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_val))\n",
    "print(y_val.shape)\n",
    "print(16/23)\n",
    "print(17/23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches for training: 0\n",
      "epoch 1: valid acc = 0.30434782608695654, new learning rate = 0.0095\n",
      "epoch 2: valid acc = 0.30434782608695654, new learning rate = 0.009025\n",
      "epoch 3: valid acc = 0.30434782608695654, new learning rate = 0.00857375\n",
      "epoch 4: valid acc = 0.30434782608695654, new learning rate = 0.0081450625\n",
      "epoch 5: valid acc = 0.30434782608695654, new learning rate = 0.007737809374999999\n",
      "epoch 6: valid acc = 0.30434782608695654, new learning rate = 0.007350918906249998\n",
      "epoch 7: valid acc = 0.30434782608695654, new learning rate = 0.006983372960937498\n",
      "epoch 8: valid acc = 0.30434782608695654, new learning rate = 0.006634204312890623\n",
      "epoch 9: valid acc = 0.30434782608695654, new learning rate = 0.006302494097246091\n",
      "epoch 10: valid acc = 0.30434782608695654, new learning rate = 0.005987369392383786\n"
     ]
    }
   ],
   "source": [
    "# Here is an example on how to collect loss and accuracy info\n",
    "\n",
    "from CNN_codes.MLPtest import MLP\n",
    "from ecbm4040.optimizers import AdamOptim\n",
    "dropout_config = {\"enabled\":True, \"keep_prob\": 0.7}\n",
    "model = MLP(input_dim=7993, hidden_dims=[5000,1000], num_classes=2, \n",
    "            weight_scale=1e-2, l2_reg=0.0, dropout_config=dropout_config)\n",
    "optimizer = AdamOptim(model)\n",
    "# hist_no_dropout contains loss, train acc and valid acc history.\n",
    "hist_no_dropout = optimizer.train(model, X_train, y_train, X_val, y_val, \n",
    "                           num_epoch=10, batch_size=90, learning_rate=1e-2, learning_decay=0.95, \n",
    "                           verbose=False, record_interval = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0552d35b29a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_acc' is not defined"
     ]
    }
   ],
   "source": [
    "print(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 7993)\n",
      "(6, 208)\n",
      "Train data shape:  (180, 7993)\n",
      "Train labels shape:  (180,)\n",
      "Validation data shape:  (28, 7993)\n",
      "Validation labels shape:  (28,)\n"
     ]
    }
   ],
   "source": [
    "# load data including nan\n",
    "\n",
    "features = np.loadtxt(open(\"braindata/Y_rho_03.csv\", \"rb\"), delimiter=\",\", skiprows=0)\n",
    "features = features.transpose()\n",
    "features = stats.zscore(features)\n",
    "print(features.shape)\n",
    "\n",
    "#y = np.loadtxt(open(\"../braindata/dx2.csv\", \"rb\"), delimiter=\",\", skiprows=1,usecols=range(1,209))\n",
    "y = np.loadtxt(open(\"braindata/dx3.csv\", \"rb\"), delimiter=\",\", skiprows=1,usecols=range(1,209))\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "## select the y label and the indices (excluding NaNs)\n",
    "y = y[4,:] \n",
    "\n",
    "X=features\n",
    "y=y\n",
    "\n",
    "# Data organizations:\n",
    "# Train data: 49000 samples from original train set: 1~49000\n",
    "# Validation data: 1000 samples from original train set: 49000~50000\n",
    "num_training = 180\n",
    "num_validation = 28\n",
    "\n",
    "X_val = X[-num_validation:, :]\n",
    "y_val = y[-num_validation:]\n",
    "\n",
    "X_train = X[:num_training, :]\n",
    "y_train = y[:num_training]\n",
    "\n",
    "# Preprocessing: subtract the mean value across every dimension for training data, and reshape it to be RGB size\n",
    "#mean_image = np.mean(X_train, axis=0)\n",
    "#X_train = X_train.astype(np.float32) - mean_image.astype(np.float32)\n",
    "#X_val = X_val.astype(np.float32) - mean_image\n",
    "\n",
    "#X_train = X_train.reshape([-1,32,32,3])/255\n",
    "#X_val = X_val.reshape([-1,32,32,3])/255\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Network Parameters: \n",
      "fc_units=[84]\n",
      "l2_norm=0.01\n",
      "seed=235\n",
      "learning_rate=0.001\n",
      "number of batches for training: 75\n",
      "epoch 1 \n",
      "Best validation accuracy! iteration:75 accuracy: 30.434782608695656%\n",
      "epoch 2 \n",
      "Best validation accuracy! iteration:150 accuracy: 52.17391304347826%\n",
      "epoch 3 \n",
      "epoch 4 \n",
      "Best validation accuracy! iteration:300 accuracy: 73.91304347826087%\n",
      "epoch 5 \n",
      "epoch 6 \n",
      "epoch 7 \n",
      "epoch 8 \n",
      "epoch 9 \n",
      "epoch 10 \n",
      "epoch 11 \n",
      "epoch 12 \n",
      "epoch 13 \n",
      "epoch 14 \n",
      "epoch 15 \n",
      "epoch 16 \n",
      "epoch 17 \n",
      "epoch 18 \n",
      "epoch 19 \n",
      "epoch 20 \n",
      "Traning ends. The best valid accuracy is 73.91304347826087. Model named mlp_1517859511.\n"
     ]
    }
   ],
   "source": [
    "from CNN_codes.MLPremake import mlp_training\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mlp_training(X_train, y_train, X_val, y_val, \n",
    "         fc_units=[84],\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-3,\n",
    "         epoch=20,\n",
    "         batch_size=1,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # load X and y\n",
    "    \n",
    "    \n",
    "    ind_num=np.isnan(y)\n",
    "    # print(ind_num.shape)\n",
    "\n",
    "\n",
    "    y_no_nan = y[~ind_num]\n",
    "\n",
    "    X_no_nan = X[~ind_num,:]\n",
    "    \n",
    "    skf=RepeatedStratifiedKFold(n_splits=cv, n_repeats=re)\n",
    "    skf.split(features, y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
